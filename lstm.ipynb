{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba\n",
    "import time\n",
    "from itertools import chain\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "valid_data = pd.read_csv('data/dev.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('data/test.tsv', sep='\\t') \n",
    "x_train, y_train = train_data.text_a.values, train_data.label.values # 训练集\n",
    "x_valid, y_valid = valid_data.text_a.values, valid_data.label.values # 验证集\n",
    "x_test, y_test = test_data.text_a.values, test_data.label.values # 测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "选择 珠江 花园 的 原因 就是 方便 ， 有 电动 扶梯 直接 到达 海边 ， 周围 餐馆 、 食廊 、 商场 、 超市 、 摊位 一应俱全 。 酒店 装修 一般 ， 但 还 算 整洁 。   泳池 在 大堂 的 屋顶 ， 因此 很小 ， 不过 女儿 倒 是 喜欢 。   包 的 早餐 是 西式 的 ， 还 算 丰富 。   服务 吗 ， 一般\n",
      "选择 珠江 花园 原因 方便 电动 扶梯 直接 到达 海边 周围 餐馆 食廊 商场 超市 摊位 一应俱全 酒店 装修 算 整洁 泳池 大堂 屋顶 很小 女儿 倒 喜欢 包 早餐 西式 算 丰富 服务\n",
      "--------------------TEXT --------------------\n",
      "Total number: 9146\n",
      "Average length: 36.85928274655587\n",
      "Max length: 13236\n",
      "Min length: 0\n",
      "Most common words : \n",
      "[('酒店', 4312), ('没有', 3197), ('房间', 3077), ('不错', 2666), ('说', 2135), ('本书', 1860), ('一个', 1848), ('感觉', 1818), ('比较', 1690), ('买', 1681), ('服务', 1601), ('书', 1522), ('住', 1487), ('非常', 1378), ('喜欢', 1192), ('入住', 1191), ('!', 1164), ('没', 1146), ('会', 1119), ('知道', 959), ('太', 958), ('有点', 957), ('觉得', 956), ('孩子', 936), ('问题', 935), ('）', 912), ('很多', 901), ('价格', 899), ('中', 892), ('（', 845), ('系统', 802), ('想', 802), ('时', 791), ('前台', 788), ('早餐', 771), ('方便', 768), ('月', 762), ('携程', 748), ('里', 722), ('服务员', 718), ('差', 682), ('发现', 681), ('不能', 670), ('东西', 652), ('看到', 649), ('现在', 648), ('宾馆', 645), ('设施', 635), ('更', 629), ('已经', 611), ('不好', 600), ('做', 600), ('元', 597), ('日', 595), ('朋友', 593), ('内容', 592), ('应该', 583), ('一本', 582), ('高', 574), ('作者', 560), ('…', 555), ('可能', 554), ('驱动', 552), ('晚上', 542), ('装', 540), ('时间', 539), ('客人', 538), ('真的', 525), ('性价比', 520), ('年', 512), ('希望', 506), ('一点', 504), ('点', 503), ('实在', 498), ('里面', 496), ('不会', 490), ('以后', 489), ('一下', 488), ('推荐', 481), ('键盘', 468), ('故事', 465), ('速度', 456), ('地方', 456), ('特别', 451), ('环境', 450), ('需要', 449), ('XP', 447), ('小时', 443), ('屏幕', 439), ('找', 436), ('建议', 435), ('写', 434), ('选择', 428), ('配置', 428), ('读', 419), ('房', 417), ('一直', 411), ('散热', 405), ('送', 402), ('外观', 402)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "cut_docs = [\" \".join(jieba.cut(x)) for x in train_data.text_a.values]\n",
    "cut_docs = list(cut_docs)\n",
    "print(cut_docs[0])\n",
    "\n",
    "# 创建停用词列表\n",
    "stopword_path = 'data/chinese_stopwords.txt'\n",
    "def get_stop_words():\n",
    "    stopwords = [line.strip() for line in open(stopword_path, encoding='UTF-8').readlines()]\n",
    "    stopwords += [\",\", \"'\"]\n",
    "    return stopwords\n",
    "stopwords = get_stop_words()\n",
    "\n",
    "# 去除停用词\n",
    "segs_without_stop = []\n",
    "for sentence in cut_docs:\n",
    "    sentWords = [x.strip() for x in sentence.split(' ') if x.strip() and x.strip() not in stopwords]\n",
    "    segs_without_stop.append(' '.join(sentWords))\n",
    "\n",
    "print(segs_without_stop[0])\n",
    "\n",
    "for doc in segs_without_stop:\n",
    "    for word in doc:\n",
    "        if word.strip():\n",
    "            vocab.add(word.strip())\n",
    "\n",
    "segs = segs_without_stop\n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "from collections import Counter\n",
    "print('-' * 20 + 'TEXT ' + '-' * 20)\n",
    "print(\"Total number: {}\".format(len(segs)))\n",
    "print(\"Average length: {}\".format(np.mean([len(sentence.split()) for sentence in segs])))\n",
    "print(\"Max length: {}\".format(np.max([len(sentence.split()) for sentence in segs])))\n",
    "print(\"Min length: {}\".format(np.min([len(sentence.split()) for sentence in segs])))\n",
    "pos_text_seg = \" \".join(segs)\n",
    "c = Counter(pos_text_seg.split()).most_common(100)\n",
    "print(\"Most common words : \\n{} \\n\".format(c))\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 将词表写入本地vocab.txt文件\n",
    "with open('data/vocab.txt', 'w') as file:\n",
    "    for word in  vocab:\n",
    "        file.write(word)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建词表\n",
    "vocab = set(vocab)\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: i + 1 for i, word in enumerate(vocab)}\n",
    "word_to_idx['<unk>'] = 0\n",
    "idx_to_word = {i + 1: word for i, word in enumerate(vocab)}\n",
    "idx_to_word[0] = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_samples(tokenized_samples, vocab):\n",
    "    features = []\n",
    "    for sample in tokenized_samples:\n",
    "        feature = []\n",
    "        for token in sample:\n",
    "            if token in word_to_idx:\n",
    "                feature.append(word_to_idx[token])\n",
    "            else:\n",
    "                feature.append(0)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "def pad_samples(features, maxlen=500, PAD=0):\n",
    "    padded_features = []\n",
    "    for feature in features:\n",
    "        if len(feature) >= maxlen:\n",
    "            padded_feature = feature[:maxlen]\n",
    "        else:\n",
    "            padded_feature = feature\n",
    "            while(len(padded_feature) < maxlen):\n",
    "                padded_feature.append(PAD)\n",
    "        padded_features.append(padded_feature)\n",
    "    return padded_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = torch.tensor(pad_samples(encode_samples(x_train, vocab)))\n",
    "train_labels = torch.tensor([score for score in y_train])\n",
    "test_features = torch.tensor(pad_samples(encode_samples(x_test, vocab)))\n",
    "test_labels = torch.tensor([score for score in y_test])\n",
    "valid_features = torch.tensor(pad_samples(encode_samples(x_valid, vocab)))\n",
    "valid_labels = torch.tensor([score for score in y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1725, 1818, 2998,  ...,    0,    0,    0],\n",
      "        [1932, 1841,  505,  ...,    0,    0,    0],\n",
      "        [3151,   61, 1240,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [2848, 2699, 2272,  ...,    0,    0,    0],\n",
      "        [2432, 1573, 1363,  ...,    0,    0,    0],\n",
      "        [1997, 2688, 1799,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(train_features)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n",
    "                 bidirectional, labels, **kwargs):\n",
    "        super(SentimentNet, self).__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,\n",
    "                               num_layers=num_layers, bidirectional=self.bidirectional,\n",
    "                               dropout=0)\n",
    "        if self.bidirectional:\n",
    "            self.decoder = nn.Linear(num_hiddens * 4, labels)\n",
    "        else:\n",
    "            self.decoder = nn.Linear(num_hiddens * 2, labels)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # states: [num_layers * num_directions, batch_size, num_hiddens],\n",
    "        # hidden: [num_layers * num_directions, batch_size, num_hiddens]\n",
    "        states, hidden = self.encoder(embeddings.permute([1, 0, 2]))\n",
    "        # encoding: [batch_size, num_hiddens]\n",
    "        encoding = torch.cat([states[0], states[-1]], dim=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs\n",
    "    \n",
    "    def single_predict(self, samples):\n",
    "        samples = encode_samples(samples, vocab)\n",
    "        padded_samples = pad_samples(samples, 500, PAD = 0)\n",
    "        padded_samples = torch.tensor(padded_samples)\n",
    "        padded_samples = padded_samples.reshape(len(samples), 1, -1)\n",
    "        padded_samples = padded_samples.to(device)\n",
    "        result_list = []\n",
    "        for i in range(len(samples)):\n",
    "            outputs = self.forward(padded_samples[i])\n",
    "            result_list.append(outputs.argmax(dim=1).item())\n",
    "        return result_list\n",
    "    \n",
    "\n",
    "        # inputs = jieba.cut(inputs)\n",
    "        # # drop the stop words\n",
    "        # inputs = [word for word in inputs if word not in stopwords]\n",
    "        # # convert the words to indices\n",
    "        # feature = []\n",
    "        # for token in inputs:\n",
    "        #     if token in word_to_idx:\n",
    "        #         feature.append(word_to_idx[token])\n",
    "        #     else:\n",
    "        #         feature.append(0)\n",
    "        # # padding\n",
    "        # if len(feature) >= 500:\n",
    "        #     padded_feature = feature[:500]\n",
    "        # else:\n",
    "        #     padded_feature = feature\n",
    "        #     while(len(padded_feature) < 500):\n",
    "        #         padded_feature.append(0)\n",
    "        # padded_feature = torch.tensor(padded_feature)\n",
    "        # padded_feature = padded_feature.reshape(1, -1)\n",
    "        # # sent to device\n",
    "        # padded_feature = padded_feature.to(device)\n",
    "        # # predict\n",
    "        # outputs = self.forward(padded_feature)\n",
    "        # return outputs.argmax(dim=1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "embed_size = 100\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "batch_size = 128\n",
    "labels = 2\n",
    "lr = 0.3\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   labels=labels)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "test_set = torch.utils.data.TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         shuffle=True)\n",
    "test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,\n",
    "                                        shuffle=False)\n",
    "valid_set = torch.utils.data.TensorDataset(valid_features, valid_labels)\n",
    "valid_iter = torch.utils.data.DataLoader(valid_set, batch_size=batch_size,\n",
    "                                            shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.6908, train acc 0.527, valid loss 0.0687, valid acc 0.560, time 35.0 sec\n",
      "epoch 2, train loss 0.6770, train acc 0.582, valid loss 0.0667, valid acc 0.581, time 35.0 sec\n",
      "epoch 3, train loss 0.6387, train acc 0.625, valid loss 0.0628, valid acc 0.657, time 35.1 sec\n",
      "epoch 4, train loss 0.6047, train acc 0.665, valid loss 0.0622, valid acc 0.639, time 35.2 sec\n",
      "epoch 5, train loss 0.5816, train acc 0.690, valid loss 0.0605, valid acc 0.650, time 35.1 sec\n",
      "epoch 6, train loss 0.5512, train acc 0.716, valid loss 0.0572, valid acc 0.720, time 35.4 sec\n",
      "epoch 7, train loss 0.5272, train acc 0.740, valid loss 0.0566, valid acc 0.732, time 35.2 sec\n",
      "epoch 8, train loss 0.5131, train acc 0.752, valid loss 0.0489, valid acc 0.749, time 35.5 sec\n",
      "epoch 9, train loss 0.4859, train acc 0.770, valid loss 0.0570, valid acc 0.722, time 36.0 sec\n",
      "epoch 10, train loss 0.4709, train acc 0.779, valid loss 0.0475, valid acc 0.749, time 36.3 sec\n",
      "epoch 11, train loss 0.4578, train acc 0.784, valid loss 0.0492, valid acc 0.799, time 35.7 sec\n",
      "epoch 12, train loss 0.4362, train acc 0.802, valid loss 0.0468, valid acc 0.801, time 35.5 sec\n",
      "epoch 13, train loss 0.4274, train acc 0.806, valid loss 0.0547, valid acc 0.765, time 35.6 sec\n",
      "epoch 14, train loss 0.4200, train acc 0.812, valid loss 0.0426, valid acc 0.830, time 35.7 sec\n",
      "epoch 15, train loss 0.3993, train acc 0.826, valid loss 0.0562, valid acc 0.747, time 35.6 sec\n",
      "epoch 16, train loss 0.4415, train acc 0.805, valid loss 0.0777, valid acc 0.628, time 35.7 sec\n",
      "epoch 17, train loss 0.3830, train acc 0.833, valid loss 0.0442, valid acc 0.820, time 35.5 sec\n",
      "epoch 18, train loss 0.3681, train acc 0.843, valid loss 0.0462, valid acc 0.833, time 35.4 sec\n",
      "epoch 19, train loss 0.3731, train acc 0.841, valid loss 0.0476, valid acc 0.816, time 35.5 sec\n",
      "epoch 20, train loss 0.3499, train acc 0.854, valid loss 0.0460, valid acc 0.845, time 35.6 sec\n",
      "epoch 21, train loss 0.2996, train acc 0.875, valid loss 0.0390, valid acc 0.851, time 35.6 sec\n",
      "epoch 22, train loss 0.3394, train acc 0.856, valid loss 0.0572, valid acc 0.786, time 35.7 sec\n",
      "epoch 23, train loss 0.3436, train acc 0.853, valid loss 0.0514, valid acc 0.828, time 35.9 sec\n",
      "epoch 24, train loss 0.3180, train acc 0.865, valid loss 0.0540, valid acc 0.808, time 35.6 sec\n",
      "epoch 25, train loss 0.2990, train acc 0.876, valid loss 0.0603, valid acc 0.784, time 35.5 sec\n",
      "epoch 26, train loss 0.2889, train acc 0.883, valid loss 0.0465, valid acc 0.836, time 35.5 sec\n",
      "epoch 27, train loss 0.2640, train acc 0.897, valid loss 0.0533, valid acc 0.821, time 35.3 sec\n",
      "epoch 28, train loss 0.2292, train acc 0.913, valid loss 0.0444, valid acc 0.849, time 35.3 sec\n",
      "epoch 29, train loss 0.2379, train acc 0.911, valid loss 0.0491, valid acc 0.829, time 35.3 sec\n",
      "epoch 30, train loss 0.2633, train acc 0.902, valid loss 0.0637, valid acc 0.783, time 35.3 sec\n",
      "epoch 31, train loss 0.2388, train acc 0.910, valid loss 0.0485, valid acc 0.832, time 35.3 sec\n",
      "epoch 32, train loss 0.1911, train acc 0.930, valid loss 0.0576, valid acc 0.804, time 35.3 sec\n",
      "epoch 33, train loss 0.1865, train acc 0.932, valid loss 0.0426, valid acc 0.855, time 35.3 sec\n",
      "epoch 34, train loss 0.4810, train acc 0.774, valid loss 0.0453, valid acc 0.831, time 35.4 sec\n",
      "epoch 35, train loss 0.2150, train acc 0.915, valid loss 0.0522, valid acc 0.845, time 35.3 sec\n",
      "epoch 36, train loss 0.1606, train acc 0.940, valid loss 0.0505, valid acc 0.847, time 35.1 sec\n",
      "epoch 37, train loss 0.1606, train acc 0.940, valid loss 0.0530, valid acc 0.849, time 35.2 sec\n",
      "epoch 38, train loss 0.1433, train acc 0.950, valid loss 0.0514, valid acc 0.854, time 35.5 sec\n",
      "epoch 39, train loss 0.1257, train acc 0.954, valid loss 0.0536, valid acc 0.850, time 35.4 sec\n",
      "epoch 40, train loss 0.1284, train acc 0.958, valid loss 0.0533, valid acc 0.842, time 35.4 sec\n",
      "epoch 41, train loss 0.1700, train acc 0.942, valid loss 0.0578, valid acc 0.827, time 35.4 sec\n",
      "epoch 42, train loss 0.2701, train acc 0.901, valid loss 0.0580, valid acc 0.804, time 35.5 sec\n",
      "epoch 43, train loss 0.1455, train acc 0.947, valid loss 0.0545, valid acc 0.853, time 35.6 sec\n",
      "epoch 44, train loss 0.1027, train acc 0.964, valid loss 0.0515, valid acc 0.843, time 35.5 sec\n",
      "epoch 45, train loss 0.1130, train acc 0.962, valid loss 0.0571, valid acc 0.857, time 35.3 sec\n",
      "epoch 46, train loss 0.1184, train acc 0.965, valid loss 0.0599, valid acc 0.859, time 35.5 sec\n",
      "epoch 47, train loss 0.0524, train acc 0.987, valid loss 0.0537, valid acc 0.858, time 35.5 sec\n",
      "epoch 48, train loss 0.0425, train acc 0.988, valid loss 0.0714, valid acc 0.856, time 35.2 sec\n",
      "epoch 49, train loss 0.0374, train acc 0.990, valid loss 0.0739, valid acc 0.858, time 35.3 sec\n",
      "epoch 50, train loss 0.0310, train acc 0.991, valid loss 0.0651, valid acc 0.855, time 35.4 sec\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, valid_losses = 0, 0\n",
    "    train_acc, valid_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "        net.zero_grad()\n",
    "        feature = feature.cuda()\n",
    "        label = label.cuda()\n",
    "        score = net(feature)\n",
    "        # print(score.type)\n",
    "        score = score.to(device=device)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        # use valid set to test\n",
    "            for valid_feature, valid_label in valid_iter:\n",
    "                m += 1\n",
    "                valid_feature = valid_feature.cuda()\n",
    "                valid_label = valid_label.cuda()\n",
    "                valid_score = net(valid_feature)\n",
    "                valid_loss = loss_function(valid_score, valid_label)\n",
    "                valid_acc += accuracy_score(torch.argmax(valid_score.cpu().data,\n",
    "                                                        dim=1), valid_label.cpu())\n",
    "        # use test set to test\n",
    "    # save the best model\n",
    "    if (valid_acc / m > 0.82) and (valid_acc / m > best_valid_acc):\n",
    "        torch.save(net, './checkpoints/best_model_bilstm.pkl')\n",
    "        best_valid_acc = valid_acc / m\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    # print the result\n",
    "    print('epoch %d, train loss %.4f, train acc %.3f, valid loss %.4f, valid acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_loss / n, train_acc / n, valid_loss / m, valid_acc / m, runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8615    0.8615    0.8615       592\n",
      "           1     0.8651    0.8651    0.8651       608\n",
      "\n",
      "    accuracy                         0.8633      1200\n",
      "   macro avg     0.8633    0.8633    0.8633      1200\n",
      "weighted avg     0.8633    0.8633    0.8633      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the best model and test with \n",
    "from sklearn.metrics import classification_report\n",
    "net_bilstm = torch.load('./checkpoints/best_model_bilstm.pkl')\n",
    "net_bilstm.eval()\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for test_feature, test_label in test_iter:\n",
    "        test_feature = test_feature.cuda()\n",
    "        test_label = test_label.cuda()\n",
    "        test_score = net_bilstm(test_feature)\n",
    "        test_acc = accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                dim=1), test_label.cpu())\n",
    "        result.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "print(classification_report(y_test, result, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_bilstm.single_predict(['房间差劲','绝绝子'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "embed_size = 200\n",
    "num_hiddens = 100\n",
    "num_layers = 2\n",
    "bidirectional = False\n",
    "batch_size = 128\n",
    "labels = 2\n",
    "lr = 0.2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net = SentimentNet(vocab_size=(vocab_size+1), embed_size=embed_size,\n",
    "                   num_hiddens=num_hiddens, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional,\n",
    "                   labels=labels)\n",
    "net.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 0.6931, train acc 0.511, valid loss 0.0693, valid acc 0.529, time 15.1 sec\n",
      "epoch 2, train loss 0.6926, train acc 0.516, valid loss 0.0694, valid acc 0.543, time 14.9 sec\n",
      "epoch 3, train loss 0.6915, train acc 0.520, valid loss 0.0696, valid acc 0.505, time 15.0 sec\n",
      "epoch 4, train loss 0.6905, train acc 0.524, valid loss 0.0697, valid acc 0.501, time 15.0 sec\n",
      "epoch 5, train loss 0.6886, train acc 0.543, valid loss 0.0698, valid acc 0.553, time 15.0 sec\n",
      "epoch 6, train loss 0.6865, train acc 0.559, valid loss 0.0701, valid acc 0.566, time 15.0 sec\n",
      "epoch 7, train loss 0.6825, train acc 0.575, valid loss 0.0704, valid acc 0.557, time 15.1 sec\n",
      "epoch 8, train loss 0.6762, train acc 0.587, valid loss 0.0709, valid acc 0.586, time 15.3 sec\n",
      "epoch 9, train loss 0.6686, train acc 0.595, valid loss 0.0708, valid acc 0.577, time 15.4 sec\n",
      "epoch 10, train loss 0.6592, train acc 0.611, valid loss 0.0711, valid acc 0.595, time 15.4 sec\n",
      "epoch 11, train loss 0.6505, train acc 0.618, valid loss 0.0698, valid acc 0.604, time 15.4 sec\n",
      "epoch 12, train loss 0.6442, train acc 0.625, valid loss 0.0690, valid acc 0.605, time 15.4 sec\n",
      "epoch 13, train loss 0.6391, train acc 0.629, valid loss 0.0675, valid acc 0.603, time 15.4 sec\n",
      "epoch 14, train loss 0.6330, train acc 0.634, valid loss 0.0680, valid acc 0.602, time 15.4 sec\n",
      "epoch 15, train loss 0.6303, train acc 0.634, valid loss 0.0665, valid acc 0.620, time 15.5 sec\n",
      "epoch 16, train loss 0.6260, train acc 0.641, valid loss 0.0658, valid acc 0.617, time 15.4 sec\n",
      "epoch 17, train loss 0.6226, train acc 0.645, valid loss 0.0645, valid acc 0.632, time 15.6 sec\n",
      "epoch 18, train loss 0.6188, train acc 0.651, valid loss 0.0648, valid acc 0.628, time 15.8 sec\n",
      "epoch 19, train loss 0.6171, train acc 0.650, valid loss 0.0619, valid acc 0.629, time 15.7 sec\n",
      "epoch 20, train loss 0.6160, train acc 0.645, valid loss 0.0645, valid acc 0.627, time 15.7 sec\n",
      "epoch 21, train loss 0.6133, train acc 0.654, valid loss 0.0635, valid acc 0.627, time 15.8 sec\n",
      "epoch 22, train loss 0.6102, train acc 0.655, valid loss 0.0631, valid acc 0.644, time 15.7 sec\n",
      "epoch 23, train loss 0.6099, train acc 0.649, valid loss 0.0630, valid acc 0.649, time 15.6 sec\n",
      "epoch 24, train loss 0.6078, train acc 0.655, valid loss 0.0664, valid acc 0.628, time 15.8 sec\n",
      "epoch 25, train loss 0.6061, train acc 0.657, valid loss 0.0642, valid acc 0.632, time 14.9 sec\n",
      "epoch 26, train loss 0.6048, train acc 0.660, valid loss 0.0638, valid acc 0.637, time 15.0 sec\n",
      "epoch 27, train loss 0.6043, train acc 0.658, valid loss 0.0642, valid acc 0.653, time 14.8 sec\n",
      "epoch 28, train loss 0.6020, train acc 0.659, valid loss 0.0605, valid acc 0.653, time 15.1 sec\n",
      "epoch 29, train loss 0.6024, train acc 0.656, valid loss 0.0636, valid acc 0.635, time 14.9 sec\n",
      "epoch 30, train loss 0.6009, train acc 0.660, valid loss 0.0610, valid acc 0.637, time 15.0 sec\n",
      "epoch 31, train loss 0.5972, train acc 0.668, valid loss 0.0655, valid acc 0.635, time 14.8 sec\n",
      "epoch 32, train loss 0.5968, train acc 0.666, valid loss 0.0612, valid acc 0.648, time 15.0 sec\n",
      "epoch 33, train loss 0.5968, train acc 0.662, valid loss 0.0633, valid acc 0.653, time 14.8 sec\n",
      "epoch 34, train loss 0.5962, train acc 0.663, valid loss 0.0640, valid acc 0.643, time 14.9 sec\n",
      "epoch 35, train loss 0.5944, train acc 0.663, valid loss 0.0626, valid acc 0.641, time 15.0 sec\n",
      "epoch 36, train loss 0.5944, train acc 0.665, valid loss 0.0634, valid acc 0.647, time 14.9 sec\n",
      "epoch 37, train loss 0.5951, train acc 0.662, valid loss 0.0653, valid acc 0.632, time 14.8 sec\n",
      "epoch 38, train loss 0.5934, train acc 0.662, valid loss 0.0635, valid acc 0.637, time 15.0 sec\n",
      "epoch 39, train loss 0.5919, train acc 0.661, valid loss 0.0629, valid acc 0.649, time 15.0 sec\n",
      "epoch 40, train loss 0.5920, train acc 0.664, valid loss 0.0620, valid acc 0.653, time 15.0 sec\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start = time.time()\n",
    "    train_loss, valid_losses = 0, 0\n",
    "    train_acc, valid_acc = 0, 0\n",
    "    n, m = 0, 0\n",
    "    for feature, label in train_iter:\n",
    "        n += 1\n",
    "        net.zero_grad()\n",
    "        feature = feature.cuda()\n",
    "        label = label.cuda()\n",
    "        score = net(feature)\n",
    "        # print(score.type)\n",
    "        score = score.to(device=device)\n",
    "        loss = loss_function(score, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += accuracy_score(torch.argmax(score.cpu().data,\n",
    "                                                 dim=1), label.cpu())\n",
    "        train_loss += loss\n",
    "    with torch.no_grad():\n",
    "        # use valid set to test\n",
    "            for valid_feature, valid_label in valid_iter:\n",
    "                m += 1\n",
    "                valid_feature = valid_feature.cuda()\n",
    "                valid_label = valid_label.cuda()\n",
    "                valid_score = net(valid_feature)\n",
    "                valid_loss = loss_function(valid_score, valid_label)\n",
    "                valid_acc += accuracy_score(torch.argmax(valid_score.cpu().data,\n",
    "                                                        dim=1), valid_label.cpu())\n",
    "        # use test set to test\n",
    "    # save the best model\n",
    "    if (valid_acc / m > 0.65) and (valid_acc / m > best_valid_acc):\n",
    "        torch.save(net, './checkpoints/best_model_lstm.pkl')\n",
    "        best_valid_acc = valid_acc / m\n",
    "    end = time.time()\n",
    "    runtime = end - start\n",
    "    # print the result\n",
    "    print('epoch %d, train loss %.4f, train acc %.3f, valid loss %.4f, valid acc %.3f, time %.1f sec'\n",
    "            % (epoch + 1, train_loss / n, train_acc / n, valid_loss / m, valid_acc / m, runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6307    0.6030    0.6166       592\n",
      "           1     0.6293    0.6562    0.6425       608\n",
      "\n",
      "    accuracy                         0.6300      1200\n",
      "   macro avg     0.6300    0.6296    0.6295      1200\n",
      "weighted avg     0.6300    0.6300    0.6297      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the best model and test with \n",
    "from sklearn.metrics import classification_report\n",
    "net_lstm = torch.load('./checkpoints/best_model_lstm.pkl')\n",
    "net_lstm.eval()\n",
    "result = []\n",
    "with torch.no_grad():\n",
    "    for test_feature, test_label in test_iter:\n",
    "        test_feature = test_feature.cuda()\n",
    "        test_label = test_label.cuda()\n",
    "        test_score = net_lstm(test_feature)\n",
    "        test_acc = accuracy_score(torch.argmax(test_score.cpu().data,\n",
    "                                                dim=1), test_label.cpu())\n",
    "        result.extend(torch.argmax(test_score.cpu().data, dim=1).numpy().tolist())\n",
    "\n",
    "print(classification_report(y_test, result, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.single_predict(['房间太小。其他的都一般。。。。。。。。。'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
