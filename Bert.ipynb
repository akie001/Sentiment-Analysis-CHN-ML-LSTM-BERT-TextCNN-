{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, BertTokenizerFast\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "\n",
    "OUTPUT_PATH = \"./checkpoints\"\n",
    "MODEL_PATH = \"./bert-base-chinese\"\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 20\n",
    "EPOCH = 5\n",
    "WEIGHT_DECAY = 0.02\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# functions\n",
    "\n",
    "def Map_Process_dataset2tensor(dataset):\n",
    "    return tokenizer(dataset['text_a'], padding='max_length', truncation = True)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "# load model\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=2)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "tokenizer.model_max_length = 200\n",
    "# read data\n",
    "train_data = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "valid_data = pd.read_csv('data/dev.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('data/test.tsv', sep='\\t')\n",
    "train_ds = datasets.Dataset.from_pandas(train_data)\n",
    "dev_ds = datasets.Dataset.from_pandas(valid_data)\n",
    "test_ds = datasets.Dataset.from_pandas(test_data)\n",
    "\n",
    "# preprocess data\n",
    "train_ds.rename_column('label', 'labels')\n",
    "dev_ds.rename_column('label', 'labels')\n",
    "test_ds.rename_column('label', 'labels')\n",
    "ds = datasets.DatasetDict({\n",
    "    \"train\": train_ds,\n",
    "    \"dev\": dev_ds,\n",
    "    \"test\": test_ds\n",
    "})\n",
    "tkz_ds = ds.map(Map_Process_dataset2tensor, batched=True)\n",
    "\n",
    "pt_ds = tkz_ds.with_format('torch')\n",
    "\n",
    "# prepare fine-tuning\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.mkdir(OUTPUT_PATH)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_PATH,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCH,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "task_evaluator = evaluate.evaluator(\"text-classification\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset = pt_ds['train'],\n",
    "    eval_dataset = pt_ds['dev'],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"checkpoints/checkpoint_best\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"checkpoints/checkpoint_best\")\n",
    "tokenizer.model_max_length = 200\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# receive input function\n",
    "\n",
    "def Infer(input_review):\n",
    "    encoded_input = tokenizer.encode(input_review, return_tensors='pt',max_length =200)\n",
    "    encoded_input = encoded_input.to(device)\n",
    "    model.to(device)\n",
    "    result = model(encoded_input)['logits']\n",
    "    label = torch.argmax(result, dim=-1)\n",
    "    return int(label)\n",
    "\n",
    "# define function to evaluate the loaded model\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    ds = dataset.map(Map_Process_dataset2tensor, batched=True)\n",
    "    pt_ds = ds.with_format('torch')\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        eval_dataset = pt_ds['test'],\n",
    "        tokenizer = tokenizer,\n",
    "        compute_metrics = compute_metrics,\n",
    "    )\n",
    "    result = trainer.evaluate()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 60/60 [01:28<00:00,  1.48s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.15233935415744781,\n",
       " 'eval_accuracy': 0.9541666666666667,\n",
       " 'eval_runtime': 90.0711,\n",
       " 'eval_samples_per_second': 13.323,\n",
       " 'eval_steps_per_second': 0.666}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the loaded model\n",
    "evaluate_model(model, tokenizer, ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "label_list = []\n",
    "\n",
    "x_test, y_test = pt_ds['test']['text_a'], pt_ds['test']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成100条\n",
      "已完成200条\n",
      "已完成300条\n",
      "已完成400条\n",
      "已完成500条\n",
      "已完成600条\n",
      "已完成700条\n",
      "已完成800条\n",
      "已完成900条\n",
      "已完成1000条\n",
      "已完成1100条\n",
      "已完成1200条\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9541666666666667"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(x_test)):\n",
    "    result = Infer(x_test[i])\n",
    "    result_list.append(result)\n",
    "    label_list.append(y_test[i])\n",
    "    if (i+1) % 100 == 0:\n",
    "        print(\"已完成{}条\".format(i+1))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(label_list, result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9559    0.9510    0.9534       592\n",
      "           1     0.9525    0.9572    0.9549       608\n",
      "\n",
      "    accuracy                         0.9542      1200\n",
      "   macro avg     0.9542    0.9541    0.9542      1200\n",
      "weighted avg     0.9542    0.9542    0.9542      1200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(label_list, result_list, digits=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实现classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_result(result_list, label_list):\n",
    "    acc = accuracy_score(label_list, result_list)\n",
    "    f1_macro = f1_score(label_list, result_list, average='macro')\n",
    "    recall_macro = recall_score(label_list, result_list, average='macro')\n",
    "    precision_macro = precision_score(label_list, result_list, average='macro')\n",
    "    f1_weighted = f1_score(label_list, result_list, average='weighted')\n",
    "    recall_weighted = recall_score(label_list, result_list, average='weighted')\n",
    "    precision_weighted = precision_score(label_list, result_list, average='weighted')\n",
    "\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for i in range(len(result_list)):\n",
    "        if result_list[i] == 1 and label_list[i] == 1:\n",
    "            TP += 1\n",
    "        elif result_list[i] == 0 and label_list[i] == 0:\n",
    "            TN += 1\n",
    "        elif result_list[i] == 1 and label_list[i] == 0:\n",
    "            FP += 1\n",
    "        elif result_list[i] == 0 and label_list[i] == 1:\n",
    "            FN += 1\n",
    "\n",
    "    # calculate precision, recall, f1-score, support of each class\n",
    "    precision_0 = TN / (TN + FP)\n",
    "    recall_0 = TN / (TN + FN)\n",
    "    f1_0 = 2 * precision_0 * recall_0 / (precision_0 + recall_0)\n",
    "    support_0 = TN + FN\n",
    "\n",
    "    precision_1 = TP / (TP + FN)\n",
    "    recall_1 = TP / (TP + FP)\n",
    "    f1_1 = 2 * precision_1 * recall_1 / (precision_1 + recall_1)\n",
    "    support_1 = TP + FP\n",
    "    \n",
    "    # print result like this\n",
    "    #         precision   recall   f1-score   support\n",
    "\n",
    "    # 负面       0.73      0.92      0.81      2444\n",
    "    # 正面       0.96      0.84      0.90      5322\n",
    "\n",
    "    # accuracy                       0.87      7766\n",
    "    # macro avg     0.84      0.88      0.85      7766\n",
    "    # weighted avg  0.89      0.87      0.87      7766\n",
    "\n",
    "    print(\"\\t\\tprecision\\trecall\\t\\tf1-score\\tsupport\")\n",
    "    print(\"负面\\t\\t%.5f\\t\\t%.5f\\t\\t%.5f\\t\\t%d\" % (precision_0, recall_0, f1_0, support_0))\n",
    "    print(\"正面\\t\\t%.5f\\t\\t%.5f\\t\\t%.5f\\t\\t%d\" % (precision_1, recall_1, f1_1, support_1))\n",
    "    print(\"\\naccuracy\\t\\t\\t\\t\\t%.5f\\t\\t%d\" % (acc,len(result_list)))\n",
    "    print(\"macro avg\\t%.5f\\t\\t%.5f\\t\\t%.5f\\t\\t%d\" % (precision_macro, recall_macro, f1_macro, len(result_list)))\n",
    "    print(\"weighted avg\\t%.5f\\t\\t%.5f\\t\\t%.5f\\t\\t%d\" % (precision_weighted, recall_weighted, f1_weighted, len(result_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tprecision\trecall\t\tf1-score\tsupport\n",
      "负面\t\t0.95101\t\t0.95586\t\t0.95343\t\t589\n",
      "正面\t\t0.95724\t\t0.95254\t\t0.95488\t\t611\n",
      "\n",
      "accuracy\t\t\t\t\t0.95417\t\t1200\n",
      "macro avg\t0.95420\t\t0.95413\t\t0.95416\t\t1200\n",
      "weighted avg\t0.95417\t\t0.95417\t\t0.95416\t\t1200\n"
     ]
    }
   ],
   "source": [
    "evaluate_result(result_list, label_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
